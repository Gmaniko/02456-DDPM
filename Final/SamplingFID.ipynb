{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from torch import nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as func\n",
    "from labml_nn.diffusion.ddpm.unet import UNet\n",
    "from DDPMfunctions import *\n",
    "from plotting_functions import *\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "network = UNet(image_channels=3,n_channels=64)\n",
    "model = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the varianche scheduler that the model is trained on, we have incorporated the name of the varianche schedule in the title of the models\n",
    "from Varianceschedule import *\n",
    "T= 1000\n",
    "schedule = 'cosine'\n",
    "beta = varianceschedule(T, schedule, device)\n",
    "alpha = 1.0-beta\n",
    "alphabar = torch.cumprod(alpha,dim=0)\n",
    "sqrt_alphabar_prod = alphabar ** 0.5\n",
    "sqrtmin_alphabar = (1 - alphabar) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a pretrained model in\n",
    "model.load_state_dict(torch.load(\"./model_saved_linear\",weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample images and save them in a folder for FID calculations\n",
    "# Directory to save generated samples\n",
    "output_dir = \"generated_samples_GOOD/epoch_cosine/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    #We have used 3000 since it takes a lot of time to sample however increasing the number of samples yields a more accurate FID calculations\n",
    "    num_samples = 3000  \n",
    "    batch_size = 100\n",
    "    #model.eval()\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "            xT = torch.randn(batch_size, 3, 32, 32).to(device)  \n",
    "            for p in range(0,T-20)[::-1]:\n",
    "                t = torch.full((batch_size,),p).long().to(device)\n",
    "                eps_pred = model(xT,t)\n",
    "                xT = backward_process(eps_pred,t[0],xT,alpha,beta,sqrtmin_alphabar,device)\n",
    "\n",
    "            xT = ((xT + 1) / 2)  \n",
    "            for j in range(xT.size(0)):\n",
    "                if j == 0:\n",
    "                    print(f\"{i}\")\n",
    "                save_image(xT[j], os.path.join(output_dir, f\"{i+j}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate FID by using the pip package \n",
    "#pip install pytorch-fid\n",
    "#Github repo: https://github.com/mseitzer/pytorch-fid\n",
    "#One can calculate the FID by running the following command in the console:\n",
    "#python -m pytorch_fid path/to/dataset1 path/to/dataset2\n",
    "\n",
    "#Make sure to have both the samples saved in a folder and the cifar/mnist saved in a folder"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
