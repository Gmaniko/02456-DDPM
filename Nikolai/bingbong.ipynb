{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WmIsFHeTlCco"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from model import ScoreNetwork0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atoICttXlCcp",
    "outputId": "e617b02d-3ff1-4ec4-e1e2-fb6ca5e502f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:06<00:00, 243kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.18MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "# Flatten the images into a vector\n",
    "# flatten = lambda x: ToTensor()(x).view(28**2)\n",
    "\n",
    "preprocess = lambda x: 2*(ToTensor()(x))-1\n",
    "\n",
    "\n",
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True,  transform=preprocess, download=True)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=preprocess)\n",
    "\n",
    "# The digit classes to use\n",
    "classes = list(range(10))\n",
    "\n",
    "def stratified_sampler(labels):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "eval_batch_size = 100\n",
    "# The loaders perform the actual work\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.targets))\n",
    "test_loader  = DataLoader(dset_test, batch_size=eval_batch_size,\n",
    "                          sampler=stratified_sampler(dset_test.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "JYxfjdHYlCcq",
    "outputId": "4e6ac56f-f754-436d-9f18-cce33c66d546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ebb92df0280>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = next(iter(train_loader))[0][0].numpy().squeeze()\n",
    "plt.imshow(im)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhudTo8ylCcq"
   },
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h0SsRL0UlCcr"
   },
   "outputs": [],
   "source": [
    "data_dim = (1, 28, 28)\n",
    "T = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, T, device=device)\n",
    "alpha = 1 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py1CeCLTlCcr"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uNJTqVL5lCcr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2nwz7cIClCcr"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss(model: ScoreNetwork0, loss_fn: nn.MSELoss, x: Tensor) -> Tensor:\n",
    "    t = torch.randint(0, T, size=(x.shape[0],1), device=device)\n",
    "    t = t[..., None, None].expand_as(x)\n",
    "    ab_t = alpha_bar[t]\n",
    "    eps = torch.randn_like(x, device=device)\n",
    "    s = torch.sqrt(ab_t)*x + torch.sqrt(1-ab_t)*eps\n",
    "    out = model(s, t)\n",
    "    return loss_fn(eps, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcKpwAdXlCcs"
   },
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T5LDXP5flCcs"
   },
   "outputs": [],
   "source": [
    "def sampling(model: ScoreNetwork0):\n",
    "    x = torch.randn(size=data_dim, device=device)\n",
    "    for t in range(T-1, -1, -1):\n",
    "        z = torch.randn(size=data_dim) if t > 0 else torch.zeros(size=data_dim)\n",
    "        z = z.to(device)\n",
    "        t_tensor = t*torch.ones_like(x, device=device)\n",
    "        out = model(x, t_tensor)\n",
    "        sd = torch.sqrt(beta[t])\n",
    "        x = 1/torch.sqrt(alpha[t])*(x - beta[t]/torch.sqrt(1-alpha_bar[t])*out) + sd*z\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2CxvUR5lCct"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LTBfQ8c6lCct"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader: DataLoader, model: ScoreNetwork0, loss_fn: nn.MSELoss, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        loss = calc_loss(model, loss_fn, X.to(device))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # test_loss, correct = 0, 0\n",
    "    test_loss = 0\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            L = calc_loss(model, loss_fn, X.to(device))\n",
    "            test_loss += L.item()\n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpdfiGZdlCcu",
    "outputId": "2053164d-a90e-45d9-e4e4-296a491622bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 239.570343  [   64/60000]\n",
      "loss: 0.966893  [ 6464/60000]\n",
      "loss: 0.882220  [12864/60000]\n",
      "loss: 0.828990  [19264/60000]\n",
      "loss: 0.767576  [25664/60000]\n",
      "loss: 0.715924  [32064/60000]\n",
      "loss: 0.563644  [38464/60000]\n",
      "loss: 0.420031  [44864/60000]\n",
      "loss: 0.405564  [51264/60000]\n",
      "loss: 0.316083  [57664/60000]\n",
      "Test Loss: 0.330109 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.333895  [   64/60000]\n",
      "loss: 0.306499  [ 6464/60000]\n",
      "loss: 0.263654  [12864/60000]\n",
      "loss: 0.259736  [19264/60000]\n",
      "loss: 0.259991  [25664/60000]\n",
      "loss: 0.192605  [32064/60000]\n",
      "loss: 0.236744  [38464/60000]\n",
      "loss: 0.181159  [44864/60000]\n",
      "loss: 0.229515  [51264/60000]\n",
      "loss: 0.197612  [57664/60000]\n",
      "Test Loss: 0.177000 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.144969  [   64/60000]\n",
      "loss: 0.194491  [ 6464/60000]\n",
      "loss: 0.154217  [12864/60000]\n",
      "loss: 0.207333  [19264/60000]\n",
      "loss: 0.150688  [25664/60000]\n",
      "loss: 0.160324  [32064/60000]\n",
      "loss: 0.143123  [38464/60000]\n",
      "loss: 0.223771  [44864/60000]\n",
      "loss: 0.138199  [51264/60000]\n",
      "loss: 0.105602  [57664/60000]\n",
      "Test Loss: 0.134002 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.138515  [   64/60000]\n",
      "loss: 0.117945  [ 6464/60000]\n",
      "loss: 0.110069  [12864/60000]\n",
      "loss: 0.114198  [19264/60000]\n",
      "loss: 0.121092  [25664/60000]\n",
      "loss: 0.086657  [32064/60000]\n",
      "loss: 0.113319  [38464/60000]\n",
      "loss: 0.135617  [44864/60000]\n",
      "loss: 0.114493  [51264/60000]\n",
      "loss: 0.085931  [57664/60000]\n",
      "Test Loss: 0.117450 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.088992  [   64/60000]\n",
      "loss: 0.099244  [ 6464/60000]\n",
      "loss: 0.158872  [12864/60000]\n",
      "loss: 0.092012  [19264/60000]\n",
      "loss: 0.157695  [25664/60000]\n",
      "loss: 0.079245  [32064/60000]\n",
      "loss: 0.083581  [38464/60000]\n",
      "loss: 0.078447  [44864/60000]\n",
      "loss: 0.076695  [51264/60000]\n",
      "loss: 0.101648  [57664/60000]\n",
      "Test Loss: 0.149206 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.162085  [   64/60000]\n",
      "loss: 0.100681  [ 6464/60000]\n",
      "loss: 0.081436  [12864/60000]\n",
      "loss: 0.101159  [19264/60000]\n",
      "loss: 0.117758  [25664/60000]\n",
      "loss: 0.067908  [32064/60000]\n",
      "loss: 0.064894  [38464/60000]\n",
      "loss: 0.089653  [44864/60000]\n",
      "loss: 0.207339  [51264/60000]\n",
      "loss: 0.077801  [57664/60000]\n",
      "Test Loss: 0.073551 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.072670  [   64/60000]\n",
      "loss: 0.084360  [ 6464/60000]\n",
      "loss: 0.074016  [12864/60000]\n",
      "loss: 0.055807  [19264/60000]\n",
      "loss: 0.069611  [25664/60000]\n",
      "loss: 0.064728  [32064/60000]\n",
      "loss: 0.089361  [38464/60000]\n",
      "loss: 0.065788  [44864/60000]\n",
      "loss: 0.058702  [51264/60000]\n",
      "loss: 0.059421  [57664/60000]\n",
      "Test Loss: 0.069862 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.079395  [   64/60000]\n",
      "loss: 0.056127  [ 6464/60000]\n",
      "loss: 0.085340  [12864/60000]\n",
      "loss: 0.057291  [19264/60000]\n",
      "loss: 0.049663  [25664/60000]\n",
      "loss: 0.054478  [32064/60000]\n",
      "loss: 0.074570  [38464/60000]\n",
      "loss: 0.086673  [44864/60000]\n",
      "loss: 0.063414  [51264/60000]\n",
      "loss: 0.060205  [57664/60000]\n",
      "Test Loss: 0.065641 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.045222  [   64/60000]\n",
      "loss: 0.071507  [ 6464/60000]\n",
      "loss: 0.062569  [12864/60000]\n",
      "loss: 0.041521  [19264/60000]\n",
      "loss: 0.076900  [25664/60000]\n",
      "loss: 0.063619  [32064/60000]\n",
      "loss: 0.072401  [38464/60000]\n",
      "loss: 0.078787  [44864/60000]\n",
      "loss: 0.052983  [51264/60000]\n",
      "loss: 0.098576  [57664/60000]\n",
      "Test Loss: 0.059824 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.048934  [   64/60000]\n",
      "loss: 0.051898  [ 6464/60000]\n",
      "loss: 0.057477  [12864/60000]\n",
      "loss: 0.225121  [19264/60000]\n",
      "loss: 0.051228  [25664/60000]\n",
      "loss: 0.040786  [32064/60000]\n",
      "loss: 0.070942  [38464/60000]\n",
      "loss: 0.055562  [44864/60000]\n",
      "loss: 0.082592  [51264/60000]\n",
      "loss: 0.043807  [57664/60000]\n",
      "Test Loss: 0.058423 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.069100  [   64/60000]\n",
      "loss: 0.049403  [ 6464/60000]\n",
      "loss: 0.071749  [12864/60000]\n",
      "loss: 0.064237  [19264/60000]\n",
      "loss: 0.053452  [25664/60000]\n",
      "loss: 0.085383  [32064/60000]\n",
      "loss: 0.062916  [38464/60000]\n",
      "loss: 0.052911  [44864/60000]\n",
      "loss: 0.061944  [51264/60000]\n",
      "loss: 0.036413  [57664/60000]\n",
      "Test Loss: 0.058117 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.051082  [   64/60000]\n",
      "loss: 0.047944  [ 6464/60000]\n",
      "loss: 0.118453  [12864/60000]\n",
      "loss: 0.057022  [19264/60000]\n",
      "loss: 0.083149  [25664/60000]\n",
      "loss: 0.062600  [32064/60000]\n",
      "loss: 0.068066  [38464/60000]\n",
      "loss: 0.039454  [44864/60000]\n",
      "loss: 0.058635  [51264/60000]\n",
      "loss: 0.066902  [57664/60000]\n",
      "Test Loss: 0.054439 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.060912  [   64/60000]\n",
      "loss: 0.052193  [ 6464/60000]\n",
      "loss: 0.057389  [12864/60000]\n",
      "loss: 0.054026  [19264/60000]\n",
      "loss: 0.080258  [25664/60000]\n",
      "loss: 0.048954  [32064/60000]\n",
      "loss: 0.050774  [38464/60000]\n",
      "loss: 0.087657  [44864/60000]\n",
      "loss: 0.060586  [51264/60000]\n",
      "loss: 0.044848  [57664/60000]\n",
      "Test Loss: 0.062466 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.063534  [   64/60000]\n",
      "loss: 0.060380  [ 6464/60000]\n",
      "loss: 0.037908  [12864/60000]\n",
      "loss: 0.065935  [19264/60000]\n",
      "loss: 0.044605  [25664/60000]\n",
      "loss: 0.046565  [32064/60000]\n",
      "loss: 0.079306  [38464/60000]\n",
      "loss: 0.084595  [44864/60000]\n",
      "loss: 0.059885  [51264/60000]\n",
      "loss: 0.047772  [57664/60000]\n",
      "Test Loss: 0.052077 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.043344  [   64/60000]\n",
      "loss: 0.067929  [ 6464/60000]\n",
      "loss: 0.071532  [12864/60000]\n",
      "loss: 0.072192  [19264/60000]\n",
      "loss: 0.041543  [25664/60000]\n",
      "loss: 0.046098  [32064/60000]\n",
      "loss: 0.137412  [38464/60000]\n",
      "loss: 0.062095  [44864/60000]\n",
      "loss: 0.063116  [51264/60000]\n",
      "loss: 0.053670  [57664/60000]\n",
      "Test Loss: 0.052128 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.042687  [   64/60000]\n",
      "loss: 0.106166  [ 6464/60000]\n",
      "loss: 0.051872  [12864/60000]\n",
      "loss: 0.038822  [19264/60000]\n",
      "loss: 0.062304  [25664/60000]\n",
      "loss: 0.060361  [32064/60000]\n",
      "loss: 0.043228  [38464/60000]\n",
      "loss: 0.073529  [44864/60000]\n",
      "loss: 0.070246  [51264/60000]\n",
      "loss: 0.073724  [57664/60000]\n",
      "Test Loss: 0.058080 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.067972  [   64/60000]\n",
      "loss: 0.039111  [ 6464/60000]\n",
      "loss: 0.039521  [12864/60000]\n",
      "loss: 0.042283  [19264/60000]\n",
      "loss: 0.075026  [25664/60000]\n",
      "loss: 0.036811  [32064/60000]\n",
      "loss: 0.056489  [38464/60000]\n",
      "loss: 0.054018  [44864/60000]\n",
      "loss: 0.040033  [51264/60000]\n",
      "loss: 0.056279  [57664/60000]\n",
      "Test Loss: 0.053003 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.062393  [   64/60000]\n",
      "loss: 0.047362  [ 6464/60000]\n",
      "loss: 0.050084  [12864/60000]\n",
      "loss: 0.043426  [19264/60000]\n",
      "loss: 0.063230  [25664/60000]\n",
      "loss: 0.053789  [32064/60000]\n",
      "loss: 0.039466  [38464/60000]\n",
      "loss: 0.055809  [44864/60000]\n",
      "loss: 0.042033  [51264/60000]\n",
      "loss: 0.062071  [57664/60000]\n",
      "Test Loss: 0.046258 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.047195  [   64/60000]\n",
      "loss: 0.070363  [ 6464/60000]\n",
      "loss: 0.048521  [12864/60000]\n",
      "loss: 0.048163  [19264/60000]\n",
      "loss: 0.050523  [25664/60000]\n",
      "loss: 0.091240  [32064/60000]\n",
      "loss: 0.052265  [38464/60000]\n",
      "loss: 0.046766  [44864/60000]\n",
      "loss: 0.041095  [51264/60000]\n",
      "loss: 0.032137  [57664/60000]\n",
      "Test Loss: 0.051963 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.043979  [   64/60000]\n",
      "loss: 0.040425  [ 6464/60000]\n",
      "loss: 0.051812  [12864/60000]\n",
      "loss: 0.039215  [19264/60000]\n",
      "loss: 0.049072  [25664/60000]\n",
      "loss: 0.069865  [32064/60000]\n",
      "loss: 0.026147  [38464/60000]\n",
      "loss: 0.053663  [44864/60000]\n",
      "loss: 0.053600  [51264/60000]\n",
      "loss: 0.046130  [57664/60000]\n",
      "Test Loss: 0.045366 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.050616  [   64/60000]\n",
      "loss: 0.045135  [ 6464/60000]\n",
      "loss: 0.074655  [12864/60000]\n",
      "loss: 0.044402  [19264/60000]\n",
      "loss: 0.051403  [25664/60000]\n",
      "loss: 0.036471  [32064/60000]\n",
      "loss: 0.045957  [38464/60000]\n",
      "loss: 0.044278  [44864/60000]\n",
      "loss: 0.036149  [51264/60000]\n",
      "loss: 0.051510  [57664/60000]\n",
      "Test Loss: 0.102458 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.095583  [   64/60000]\n",
      "loss: 0.033283  [ 6464/60000]\n",
      "loss: 0.031566  [12864/60000]\n",
      "loss: 0.040664  [19264/60000]\n",
      "loss: 0.035909  [25664/60000]\n",
      "loss: 0.034072  [32064/60000]\n",
      "loss: 0.033139  [38464/60000]\n",
      "loss: 0.036777  [44864/60000]\n",
      "loss: 0.042245  [51264/60000]\n",
      "loss: 0.036202  [57664/60000]\n",
      "Test Loss: 0.048966 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.046402  [   64/60000]\n",
      "loss: 0.033572  [ 6464/60000]\n",
      "loss: 0.050884  [12864/60000]\n",
      "loss: 0.046597  [19264/60000]\n",
      "loss: 0.050609  [25664/60000]\n",
      "loss: 0.042070  [32064/60000]\n",
      "loss: 0.052108  [38464/60000]\n",
      "loss: 0.029523  [44864/60000]\n",
      "loss: 0.034104  [51264/60000]\n",
      "loss: 0.031580  [57664/60000]\n",
      "Test Loss: 0.043226 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.032016  [   64/60000]\n",
      "loss: 0.039210  [ 6464/60000]\n",
      "loss: 0.034010  [12864/60000]\n",
      "loss: 0.054074  [19264/60000]\n",
      "loss: 0.037419  [25664/60000]\n",
      "loss: 0.042328  [32064/60000]\n",
      "loss: 0.030771  [38464/60000]\n",
      "loss: 0.042330  [44864/60000]\n",
      "loss: 0.036489  [51264/60000]\n",
      "loss: 0.047725  [57664/60000]\n",
      "Test Loss: 0.043415 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.040374  [   64/60000]\n",
      "loss: 0.040586  [ 6464/60000]\n",
      "loss: 0.045784  [12864/60000]\n",
      "loss: 0.057162  [19264/60000]\n",
      "loss: 0.052487  [25664/60000]\n",
      "loss: 0.035073  [32064/60000]\n",
      "loss: 0.054510  [38464/60000]\n",
      "loss: 0.030745  [44864/60000]\n",
      "loss: 0.069063  [51264/60000]\n",
      "loss: 0.033741  [57664/60000]\n",
      "Test Loss: 0.042464 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.059371  [   64/60000]\n",
      "loss: 0.039540  [ 6464/60000]\n",
      "loss: 0.032322  [12864/60000]\n",
      "loss: 0.046509  [19264/60000]\n",
      "loss: 0.028045  [25664/60000]\n",
      "loss: 0.040575  [32064/60000]\n",
      "loss: 0.033553  [38464/60000]\n",
      "loss: 0.031905  [44864/60000]\n",
      "loss: 0.040231  [51264/60000]\n",
      "loss: 0.029639  [57664/60000]\n",
      "Test Loss: 0.049557 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.045662  [   64/60000]\n",
      "loss: 0.028678  [ 6464/60000]\n",
      "loss: 0.062423  [12864/60000]\n",
      "loss: 0.041186  [19264/60000]\n",
      "loss: 0.041800  [25664/60000]\n",
      "loss: 0.057591  [32064/60000]\n",
      "loss: 0.042998  [38464/60000]\n",
      "loss: 0.033648  [44864/60000]\n",
      "loss: 0.040818  [51264/60000]\n",
      "loss: 0.050400  [57664/60000]\n",
      "Test Loss: 0.039460 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.029703  [   64/60000]\n",
      "loss: 0.034245  [ 6464/60000]\n",
      "loss: 0.038794  [12864/60000]\n",
      "loss: 0.044742  [19264/60000]\n",
      "loss: 0.045745  [25664/60000]\n",
      "loss: 0.047352  [32064/60000]\n",
      "loss: 0.044035  [38464/60000]\n",
      "loss: 0.030362  [44864/60000]\n",
      "loss: 0.049818  [51264/60000]\n",
      "loss: 0.050138  [57664/60000]\n",
      "Test Loss: 0.039098 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.024372  [   64/60000]\n",
      "loss: 0.039822  [ 6464/60000]\n",
      "loss: 0.041839  [12864/60000]\n",
      "loss: 0.032984  [19264/60000]\n",
      "loss: 0.044224  [25664/60000]\n",
      "loss: 0.044372  [32064/60000]\n",
      "loss: 0.035002  [38464/60000]\n",
      "loss: 0.036573  [44864/60000]\n",
      "loss: 0.026283  [51264/60000]\n",
      "loss: 0.039997  [57664/60000]\n",
      "Test Loss: 0.038421 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.038743  [   64/60000]\n",
      "loss: 0.044990  [ 6464/60000]\n",
      "loss: 0.042701  [12864/60000]\n",
      "loss: 0.037247  [19264/60000]\n",
      "loss: 0.032295  [25664/60000]\n",
      "loss: 0.039411  [32064/60000]\n",
      "loss: 0.037185  [38464/60000]\n",
      "loss: 0.038629  [44864/60000]\n",
      "loss: 0.025613  [51264/60000]\n",
      "loss: 0.039128  [57664/60000]\n",
      "Test Loss: 0.043881 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.038835  [   64/60000]\n",
      "loss: 0.060626  [ 6464/60000]\n",
      "loss: 0.028738  [12864/60000]\n",
      "loss: 0.036667  [19264/60000]\n",
      "loss: 0.043813  [25664/60000]\n",
      "loss: 0.037531  [32064/60000]\n",
      "loss: 0.051169  [38464/60000]\n",
      "loss: 0.033410  [44864/60000]\n",
      "loss: 0.045473  [51264/60000]\n",
      "loss: 0.045199  [57664/60000]\n",
      "Test Loss: 0.036981 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.049478  [   64/60000]\n",
      "loss: 0.049112  [ 6464/60000]\n",
      "loss: 0.035109  [12864/60000]\n",
      "loss: 0.022362  [19264/60000]\n",
      "loss: 0.033871  [25664/60000]\n",
      "loss: 0.044096  [32064/60000]\n",
      "loss: 0.038126  [38464/60000]\n",
      "loss: 0.032740  [44864/60000]\n",
      "loss: 0.024713  [51264/60000]\n",
      "loss: 0.036166  [57664/60000]\n",
      "Test Loss: 0.039793 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.034955  [   64/60000]\n",
      "loss: 0.026944  [ 6464/60000]\n",
      "loss: 0.054527  [12864/60000]\n",
      "loss: 0.041588  [19264/60000]\n",
      "loss: 0.041755  [25664/60000]\n",
      "loss: 0.037485  [32064/60000]\n",
      "loss: 0.037746  [38464/60000]\n",
      "loss: 0.036466  [44864/60000]\n",
      "loss: 0.042866  [51264/60000]\n",
      "loss: 0.046049  [57664/60000]\n",
      "Test Loss: 0.037780 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.035221  [   64/60000]\n",
      "loss: 0.031338  [ 6464/60000]\n",
      "loss: 0.033264  [12864/60000]\n",
      "loss: 0.043429  [19264/60000]\n",
      "loss: 0.038133  [25664/60000]\n",
      "loss: 0.037468  [32064/60000]\n",
      "loss: 0.053678  [38464/60000]\n",
      "loss: 0.034717  [44864/60000]\n",
      "loss: 0.044953  [51264/60000]\n",
      "loss: 0.030843  [57664/60000]\n",
      "Test Loss: 0.037922 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.033964  [   64/60000]\n",
      "loss: 0.033094  [ 6464/60000]\n",
      "loss: 0.029748  [12864/60000]\n",
      "loss: 0.026622  [19264/60000]\n",
      "loss: 0.033687  [25664/60000]\n",
      "loss: 0.036076  [32064/60000]\n",
      "loss: 0.034812  [38464/60000]\n",
      "loss: 0.033559  [44864/60000]\n",
      "loss: 0.042446  [51264/60000]\n",
      "loss: 0.024718  [57664/60000]\n",
      "Test Loss: 0.037785 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.028267  [   64/60000]\n",
      "loss: 0.037904  [ 6464/60000]\n",
      "loss: 0.036809  [12864/60000]\n",
      "loss: 0.031292  [19264/60000]\n",
      "loss: 0.055166  [25664/60000]\n",
      "loss: 0.035513  [32064/60000]\n",
      "loss: 0.040212  [38464/60000]\n",
      "loss: 0.045370  [44864/60000]\n",
      "loss: 0.028253  [51264/60000]\n",
      "loss: 0.036068  [57664/60000]\n",
      "Test Loss: 0.037106 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.047811  [   64/60000]\n",
      "loss: 0.041656  [ 6464/60000]\n",
      "loss: 0.026029  [12864/60000]\n",
      "loss: 0.023005  [19264/60000]\n",
      "loss: 0.039435  [25664/60000]\n",
      "loss: 0.046918  [32064/60000]\n",
      "loss: 0.045433  [38464/60000]\n",
      "loss: 0.035122  [44864/60000]\n",
      "loss: 0.024930  [51264/60000]\n",
      "loss: 0.039472  [57664/60000]\n",
      "Test Loss: 0.034092 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.030164  [   64/60000]\n",
      "loss: 0.055827  [ 6464/60000]\n",
      "loss: 0.039986  [12864/60000]\n",
      "loss: 0.034464  [19264/60000]\n",
      "loss: 0.025492  [25664/60000]\n",
      "loss: 0.031416  [32064/60000]\n",
      "loss: 0.035336  [38464/60000]\n",
      "loss: 0.045609  [44864/60000]\n",
      "loss: 0.045067  [51264/60000]\n",
      "loss: 0.035942  [57664/60000]\n",
      "Test Loss: 0.036181 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.031161  [   64/60000]\n",
      "loss: 0.031622  [ 6464/60000]\n",
      "loss: 0.032164  [12864/60000]\n",
      "loss: 0.022328  [19264/60000]\n",
      "loss: 0.045028  [25664/60000]\n",
      "loss: 0.029005  [32064/60000]\n",
      "loss: 0.033703  [38464/60000]\n",
      "loss: 0.030480  [44864/60000]\n",
      "loss: 0.034208  [51264/60000]\n",
      "loss: 0.028704  [57664/60000]\n",
      "Test Loss: 0.033568 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.035557  [   64/60000]\n",
      "loss: 0.034273  [ 6464/60000]\n",
      "loss: 0.040740  [12864/60000]\n",
      "loss: 0.024294  [19264/60000]\n",
      "loss: 0.034919  [25664/60000]\n",
      "loss: 0.037256  [32064/60000]\n",
      "loss: 0.036622  [38464/60000]\n",
      "loss: 0.049859  [44864/60000]\n",
      "loss: 0.045143  [51264/60000]\n",
      "loss: 0.026940  [57664/60000]\n",
      "Test Loss: 0.034794 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.030508  [   64/60000]\n",
      "loss: 0.024186  [ 6464/60000]\n",
      "loss: 0.050522  [12864/60000]\n",
      "loss: 0.023051  [19264/60000]\n",
      "loss: 0.036232  [25664/60000]\n",
      "loss: 0.030311  [32064/60000]\n",
      "loss: 0.030175  [38464/60000]\n",
      "loss: 0.042595  [44864/60000]\n",
      "loss: 0.033405  [51264/60000]\n",
      "loss: 0.040826  [57664/60000]\n",
      "Test Loss: 0.033527 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.043078  [   64/60000]\n",
      "loss: 0.032875  [ 6464/60000]\n",
      "loss: 0.025221  [12864/60000]\n",
      "loss: 0.025083  [19264/60000]\n",
      "loss: 0.042116  [25664/60000]\n",
      "loss: 0.030146  [32064/60000]\n",
      "loss: 0.048882  [38464/60000]\n",
      "loss: 0.020653  [44864/60000]\n",
      "loss: 0.039538  [51264/60000]\n",
      "loss: 0.037583  [57664/60000]\n",
      "Test Loss: 0.036130 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.043361  [   64/60000]\n",
      "loss: 0.024984  [ 6464/60000]\n",
      "loss: 0.030215  [12864/60000]\n",
      "loss: 0.031738  [19264/60000]\n",
      "loss: 0.022963  [25664/60000]\n",
      "loss: 0.018405  [32064/60000]\n",
      "loss: 0.026520  [38464/60000]\n",
      "loss: 0.035520  [44864/60000]\n",
      "loss: 0.026093  [51264/60000]\n",
      "loss: 0.042654  [57664/60000]\n",
      "Test Loss: 0.034573 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.034183  [   64/60000]\n",
      "loss: 0.034997  [ 6464/60000]\n",
      "loss: 0.052128  [12864/60000]\n",
      "loss: 0.018377  [19264/60000]\n",
      "loss: 0.029890  [25664/60000]\n",
      "loss: 0.031552  [32064/60000]\n",
      "loss: 0.036416  [38464/60000]\n",
      "loss: 0.041200  [44864/60000]\n",
      "loss: 0.034591  [51264/60000]\n",
      "loss: 0.024901  [57664/60000]\n",
      "Test Loss: 0.032871 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.025591  [   64/60000]\n",
      "loss: 0.047838  [ 6464/60000]\n",
      "loss: 0.025746  [12864/60000]\n",
      "loss: 0.047217  [19264/60000]\n",
      "loss: 0.027603  [25664/60000]\n",
      "loss: 0.049022  [32064/60000]\n",
      "loss: 0.037602  [38464/60000]\n",
      "loss: 0.022347  [44864/60000]\n",
      "loss: 0.037720  [51264/60000]\n",
      "loss: 0.042536  [57664/60000]\n",
      "Test Loss: 0.031903 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.026344  [   64/60000]\n",
      "loss: 0.025946  [ 6464/60000]\n",
      "loss: 0.042485  [12864/60000]\n",
      "loss: 0.030938  [19264/60000]\n",
      "loss: 0.033689  [25664/60000]\n",
      "loss: 0.041329  [32064/60000]\n",
      "loss: 0.030032  [38464/60000]\n",
      "loss: 0.035198  [44864/60000]\n",
      "loss: 0.048275  [51264/60000]\n",
      "loss: 0.031393  [57664/60000]\n",
      "Test Loss: 0.033510 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.036840  [   64/60000]\n",
      "loss: 0.037402  [ 6464/60000]\n",
      "loss: 0.042906  [12864/60000]\n",
      "loss: 0.049613  [19264/60000]\n",
      "loss: 0.027320  [25664/60000]\n",
      "loss: 0.035082  [32064/60000]\n",
      "loss: 0.028639  [38464/60000]\n",
      "loss: 0.028323  [44864/60000]\n",
      "loss: 0.050710  [51264/60000]\n",
      "loss: 0.038459  [57664/60000]\n",
      "Test Loss: 0.034189 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.035316  [   64/60000]\n",
      "loss: 0.047487  [ 6464/60000]\n",
      "loss: 0.026675  [12864/60000]\n",
      "loss: 0.031144  [19264/60000]\n",
      "loss: 0.033840  [25664/60000]\n",
      "loss: 0.035039  [32064/60000]\n",
      "loss: 0.040395  [38464/60000]\n",
      "loss: 0.027230  [44864/60000]\n",
      "loss: 0.021108  [51264/60000]\n",
      "loss: 0.027442  [57664/60000]\n",
      "Test Loss: 0.036673 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.038865  [   64/60000]\n",
      "loss: 0.027520  [ 6464/60000]\n",
      "loss: 0.028617  [12864/60000]\n",
      "loss: 0.033862  [19264/60000]\n",
      "loss: 0.014417  [25664/60000]\n",
      "loss: 0.030540  [32064/60000]\n",
      "loss: 0.038795  [38464/60000]\n",
      "loss: 0.031919  [44864/60000]\n",
      "loss: 0.026482  [51264/60000]\n",
      "loss: 0.039097  [57664/60000]\n",
      "Test Loss: 0.032667 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.024433  [   64/60000]\n",
      "loss: 0.028164  [ 6464/60000]\n",
      "loss: 0.024062  [12864/60000]\n",
      "loss: 0.032246  [19264/60000]\n",
      "loss: 0.033808  [25664/60000]\n",
      "loss: 0.029791  [32064/60000]\n",
      "loss: 0.025663  [38464/60000]\n",
      "loss: 0.047139  [44864/60000]\n",
      "loss: 0.034684  [51264/60000]\n",
      "loss: 0.029526  [57664/60000]\n",
      "Test Loss: 0.032050 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.029295  [   64/60000]\n",
      "loss: 0.029200  [ 6464/60000]\n",
      "loss: 0.034104  [12864/60000]\n",
      "loss: 0.043827  [19264/60000]\n",
      "loss: 0.034572  [25664/60000]\n",
      "loss: 0.023985  [32064/60000]\n",
      "loss: 0.030926  [38464/60000]\n",
      "loss: 0.033316  [44864/60000]\n",
      "loss: 0.024010  [51264/60000]\n",
      "loss: 0.041194  [57664/60000]\n",
      "Test Loss: 0.032138 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.037344  [   64/60000]\n",
      "loss: 0.036238  [ 6464/60000]\n",
      "loss: 0.024847  [12864/60000]\n",
      "loss: 0.049106  [19264/60000]\n",
      "loss: 0.036321  [25664/60000]\n",
      "loss: 0.034988  [32064/60000]\n",
      "loss: 0.025992  [38464/60000]\n",
      "loss: 0.043772  [44864/60000]\n",
      "loss: 0.032064  [51264/60000]\n",
      "loss: 0.027739  [57664/60000]\n",
      "Test Loss: 0.030638 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.033815  [   64/60000]\n",
      "loss: 0.024792  [ 6464/60000]\n",
      "loss: 0.041613  [12864/60000]\n",
      "loss: 0.034380  [19264/60000]\n",
      "loss: 0.037344  [25664/60000]\n",
      "loss: 0.044591  [32064/60000]\n",
      "loss: 0.037957  [38464/60000]\n",
      "loss: 0.036809  [44864/60000]\n",
      "loss: 0.035334  [51264/60000]\n",
      "loss: 0.032132  [57664/60000]\n",
      "Test Loss: 0.031856 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.028595  [   64/60000]\n",
      "loss: 0.036260  [ 6464/60000]\n",
      "loss: 0.026859  [12864/60000]\n",
      "loss: 0.022602  [19264/60000]\n",
      "loss: 0.046954  [25664/60000]\n",
      "loss: 0.046763  [32064/60000]\n",
      "loss: 0.033980  [38464/60000]\n",
      "loss: 0.026589  [44864/60000]\n",
      "loss: 0.032092  [51264/60000]\n",
      "loss: 0.037368  [57664/60000]\n",
      "Test Loss: 0.029865 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.018566  [   64/60000]\n",
      "loss: 0.018387  [ 6464/60000]\n",
      "loss: 0.041200  [12864/60000]\n",
      "loss: 0.038364  [19264/60000]\n",
      "loss: 0.039954  [25664/60000]\n",
      "loss: 0.029082  [32064/60000]\n",
      "loss: 0.050059  [38464/60000]\n",
      "loss: 0.032542  [44864/60000]\n",
      "loss: 0.035009  [51264/60000]\n",
      "loss: 0.039405  [57664/60000]\n",
      "Test Loss: 0.031994 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.035810  [   64/60000]\n",
      "loss: 0.027170  [ 6464/60000]\n",
      "loss: 0.031990  [12864/60000]\n",
      "loss: 0.031001  [19264/60000]\n",
      "loss: 0.035960  [25664/60000]\n",
      "loss: 0.024632  [32064/60000]\n",
      "loss: 0.030084  [38464/60000]\n",
      "loss: 0.061523  [44864/60000]\n",
      "loss: 0.033477  [51264/60000]\n",
      "loss: 0.030596  [57664/60000]\n",
      "Test Loss: 0.031109 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.028494  [   64/60000]\n",
      "loss: 0.032955  [ 6464/60000]\n",
      "loss: 0.031295  [12864/60000]\n",
      "loss: 0.033175  [19264/60000]\n",
      "loss: 0.032098  [25664/60000]\n",
      "loss: 0.031948  [32064/60000]\n",
      "loss: 0.028292  [38464/60000]\n",
      "loss: 0.026910  [44864/60000]\n",
      "loss: 0.038071  [51264/60000]\n",
      "loss: 0.032426  [57664/60000]\n",
      "Test Loss: 0.030336 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.032673  [   64/60000]\n",
      "loss: 0.029425  [ 6464/60000]\n",
      "loss: 0.027868  [12864/60000]\n",
      "loss: 0.022587  [19264/60000]\n",
      "loss: 0.036319  [25664/60000]\n",
      "loss: 0.019989  [32064/60000]\n",
      "loss: 0.024753  [38464/60000]\n",
      "loss: 0.030539  [44864/60000]\n",
      "loss: 0.032060  [51264/60000]\n",
      "loss: 0.038129  [57664/60000]\n",
      "Test Loss: 0.031233 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.049249  [   64/60000]\n",
      "loss: 0.030597  [ 6464/60000]\n",
      "loss: 0.030542  [12864/60000]\n",
      "loss: 0.031432  [19264/60000]\n",
      "loss: 0.031613  [25664/60000]\n",
      "loss: 0.036488  [32064/60000]\n",
      "loss: 0.028399  [38464/60000]\n",
      "loss: 0.031220  [44864/60000]\n",
      "loss: 0.036148  [51264/60000]\n",
      "loss: 0.022915  [57664/60000]\n",
      "Test Loss: 0.031002 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.048689  [   64/60000]\n",
      "loss: 0.032508  [ 6464/60000]\n",
      "loss: 0.027294  [12864/60000]\n",
      "loss: 0.027965  [19264/60000]\n",
      "loss: 0.033903  [25664/60000]\n",
      "loss: 0.022106  [32064/60000]\n",
      "loss: 0.021412  [38464/60000]\n",
      "loss: 0.032407  [44864/60000]\n",
      "loss: 0.025546  [51264/60000]\n",
      "loss: 0.028601  [57664/60000]\n",
      "Test Loss: 0.030022 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.024205  [   64/60000]\n",
      "loss: 0.042642  [ 6464/60000]\n",
      "loss: 0.031666  [12864/60000]\n",
      "loss: 0.035941  [19264/60000]\n",
      "loss: 0.022655  [25664/60000]\n",
      "loss: 0.042745  [32064/60000]\n",
      "loss: 0.029343  [38464/60000]\n",
      "loss: 0.031816  [44864/60000]\n",
      "loss: 0.028691  [51264/60000]\n",
      "loss: 0.032397  [57664/60000]\n",
      "Test Loss: 0.030396 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.027940  [   64/60000]\n",
      "loss: 0.032741  [ 6464/60000]\n",
      "loss: 0.032957  [12864/60000]\n",
      "loss: 0.027877  [19264/60000]\n",
      "loss: 0.028639  [25664/60000]\n",
      "loss: 0.033740  [32064/60000]\n",
      "loss: 0.034789  [38464/60000]\n",
      "loss: 0.025302  [44864/60000]\n",
      "loss: 0.020345  [51264/60000]\n",
      "loss: 0.033594  [57664/60000]\n",
      "Test Loss: 0.029573 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.038832  [   64/60000]\n",
      "loss: 0.030212  [ 6464/60000]\n",
      "loss: 0.038185  [12864/60000]\n",
      "loss: 0.032841  [19264/60000]\n",
      "loss: 0.024564  [25664/60000]\n",
      "loss: 0.023496  [32064/60000]\n",
      "loss: 0.026443  [38464/60000]\n",
      "loss: 0.024435  [44864/60000]\n",
      "loss: 0.032582  [51264/60000]\n",
      "loss: 0.038148  [57664/60000]\n",
      "Test Loss: 0.029500 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.028250  [   64/60000]\n",
      "loss: 0.029277  [ 6464/60000]\n",
      "loss: 0.021054  [12864/60000]\n",
      "loss: 0.030841  [19264/60000]\n",
      "loss: 0.029575  [25664/60000]\n",
      "loss: 0.025513  [32064/60000]\n",
      "loss: 0.029085  [38464/60000]\n",
      "loss: 0.030483  [44864/60000]\n",
      "loss: 0.031604  [51264/60000]\n",
      "loss: 0.041468  [57664/60000]\n",
      "Test Loss: 0.030259 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.031560  [   64/60000]\n",
      "loss: 0.024686  [ 6464/60000]\n",
      "loss: 0.041429  [12864/60000]\n",
      "loss: 0.030808  [19264/60000]\n",
      "loss: 0.029128  [25664/60000]\n",
      "loss: 0.022616  [32064/60000]\n",
      "loss: 0.024963  [38464/60000]\n",
      "loss: 0.030355  [44864/60000]\n",
      "loss: 0.025541  [51264/60000]\n",
      "loss: 0.022627  [57664/60000]\n",
      "Test Loss: 0.029772 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.027951  [   64/60000]\n",
      "loss: 0.032488  [ 6464/60000]\n",
      "loss: 0.039172  [12864/60000]\n",
      "loss: 0.030398  [19264/60000]\n",
      "loss: 0.028101  [25664/60000]\n",
      "loss: 0.032622  [32064/60000]\n",
      "loss: 0.029547  [38464/60000]\n",
      "loss: 0.030323  [44864/60000]\n",
      "loss: 0.022233  [51264/60000]\n",
      "loss: 0.026415  [57664/60000]\n",
      "Test Loss: 0.029416 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.041886  [   64/60000]\n",
      "loss: 0.029082  [ 6464/60000]\n",
      "loss: 0.026368  [12864/60000]\n",
      "loss: 0.042894  [19264/60000]\n",
      "loss: 0.037020  [25664/60000]\n",
      "loss: 0.042282  [32064/60000]\n",
      "loss: 0.037723  [38464/60000]\n",
      "loss: 0.031160  [44864/60000]\n",
      "loss: 0.019492  [51264/60000]\n",
      "loss: 0.042296  [57664/60000]\n",
      "Test Loss: 0.029374 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.023181  [   64/60000]\n",
      "loss: 0.020298  [ 6464/60000]\n",
      "loss: 0.025767  [12864/60000]\n",
      "loss: 0.022546  [19264/60000]\n",
      "loss: 0.029139  [25664/60000]\n",
      "loss: 0.023881  [32064/60000]\n",
      "loss: 0.032221  [38464/60000]\n",
      "loss: 0.028040  [44864/60000]\n",
      "loss: 0.028825  [51264/60000]\n",
      "loss: 0.034879  [57664/60000]\n",
      "Test Loss: 0.028562 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.023437  [   64/60000]\n",
      "loss: 0.034029  [ 6464/60000]\n",
      "loss: 0.030841  [12864/60000]\n",
      "loss: 0.036699  [19264/60000]\n",
      "loss: 0.033687  [25664/60000]\n",
      "loss: 0.031443  [32064/60000]\n",
      "loss: 0.032916  [38464/60000]\n",
      "loss: 0.021709  [44864/60000]\n",
      "loss: 0.022569  [51264/60000]\n",
      "loss: 0.032705  [57664/60000]\n",
      "Test Loss: 0.028720 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.031346  [   64/60000]\n",
      "loss: 0.021053  [ 6464/60000]\n",
      "loss: 0.023445  [12864/60000]\n",
      "loss: 0.030115  [19264/60000]\n",
      "loss: 0.031320  [25664/60000]\n",
      "loss: 0.034166  [32064/60000]\n",
      "loss: 0.031718  [38464/60000]\n",
      "loss: 0.036708  [44864/60000]\n",
      "loss: 0.031658  [51264/60000]\n",
      "loss: 0.026773  [57664/60000]\n",
      "Test Loss: 0.030209 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.023635  [   64/60000]\n",
      "loss: 0.021383  [ 6464/60000]\n",
      "loss: 0.030146  [12864/60000]\n",
      "loss: 0.037352  [19264/60000]\n",
      "loss: 0.029522  [25664/60000]\n",
      "loss: 0.022620  [32064/60000]\n",
      "loss: 0.029271  [38464/60000]\n",
      "loss: 0.022821  [44864/60000]\n",
      "loss: 0.021993  [51264/60000]\n",
      "loss: 0.033135  [57664/60000]\n",
      "Test Loss: 0.029504 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.026329  [   64/60000]\n",
      "loss: 0.029335  [ 6464/60000]\n",
      "loss: 0.018570  [12864/60000]\n",
      "loss: 0.029792  [19264/60000]\n",
      "loss: 0.027899  [25664/60000]\n",
      "loss: 0.039057  [32064/60000]\n",
      "loss: 0.018488  [38464/60000]\n",
      "loss: 0.024223  [44864/60000]\n",
      "loss: 0.025021  [51264/60000]\n",
      "loss: 0.026829  [57664/60000]\n",
      "Test Loss: 0.029117 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.029539  [   64/60000]\n",
      "loss: 0.026430  [ 6464/60000]\n",
      "loss: 0.036574  [12864/60000]\n",
      "loss: 0.034495  [19264/60000]\n",
      "loss: 0.022638  [25664/60000]\n",
      "loss: 0.025752  [32064/60000]\n",
      "loss: 0.031736  [38464/60000]\n",
      "loss: 0.029317  [44864/60000]\n",
      "loss: 0.036280  [51264/60000]\n",
      "loss: 0.025041  [57664/60000]\n",
      "Test Loss: 0.028613 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.025957  [   64/60000]\n",
      "loss: 0.020217  [ 6464/60000]\n",
      "loss: 0.026508  [12864/60000]\n",
      "loss: 0.025562  [19264/60000]\n",
      "loss: 0.027520  [25664/60000]\n",
      "loss: 0.034669  [32064/60000]\n",
      "loss: 0.026428  [38464/60000]\n",
      "loss: 0.029355  [44864/60000]\n",
      "loss: 0.026517  [51264/60000]\n",
      "loss: 0.029208  [57664/60000]\n",
      "Test Loss: 0.028518 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.029650  [   64/60000]\n",
      "loss: 0.034315  [ 6464/60000]\n",
      "loss: 0.032353  [12864/60000]\n",
      "loss: 0.040545  [19264/60000]\n",
      "loss: 0.025856  [25664/60000]\n",
      "loss: 0.023040  [32064/60000]\n",
      "loss: 0.029761  [38464/60000]\n",
      "loss: 0.018406  [44864/60000]\n",
      "loss: 0.030784  [51264/60000]\n",
      "loss: 0.027568  [57664/60000]\n",
      "Test Loss: 0.028256 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.027510  [   64/60000]\n",
      "loss: 0.022961  [ 6464/60000]\n",
      "loss: 0.030747  [12864/60000]\n",
      "loss: 0.024818  [19264/60000]\n",
      "loss: 0.026483  [25664/60000]\n",
      "loss: 0.033880  [32064/60000]\n",
      "loss: 0.020874  [38464/60000]\n",
      "loss: 0.037039  [44864/60000]\n",
      "loss: 0.025828  [51264/60000]\n",
      "loss: 0.026227  [57664/60000]\n",
      "Test Loss: 0.028277 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.024638  [   64/60000]\n",
      "loss: 0.027631  [ 6464/60000]\n",
      "loss: 0.035269  [12864/60000]\n",
      "loss: 0.026746  [19264/60000]\n",
      "loss: 0.027407  [25664/60000]\n",
      "loss: 0.029129  [32064/60000]\n",
      "loss: 0.046389  [38464/60000]\n",
      "loss: 0.034806  [44864/60000]\n",
      "loss: 0.032286  [51264/60000]\n",
      "loss: 0.022290  [57664/60000]\n",
      "Test Loss: 0.028838 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.031992  [   64/60000]\n",
      "loss: 0.022129  [ 6464/60000]\n",
      "loss: 0.034392  [12864/60000]\n",
      "loss: 0.030843  [19264/60000]\n",
      "loss: 0.028877  [25664/60000]\n",
      "loss: 0.025574  [32064/60000]\n",
      "loss: 0.025497  [38464/60000]\n",
      "loss: 0.037843  [44864/60000]\n",
      "loss: 0.023800  [51264/60000]\n",
      "loss: 0.034373  [57664/60000]\n",
      "Test Loss: 0.029394 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.027354  [   64/60000]\n",
      "loss: 0.025340  [ 6464/60000]\n",
      "loss: 0.037296  [12864/60000]\n",
      "loss: 0.035496  [19264/60000]\n",
      "loss: 0.021786  [25664/60000]\n",
      "loss: 0.026958  [32064/60000]\n",
      "loss: 0.029683  [38464/60000]\n",
      "loss: 0.029600  [44864/60000]\n",
      "loss: 0.027918  [51264/60000]\n",
      "loss: 0.031438  [57664/60000]\n",
      "Test Loss: 0.029984 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.032676  [   64/60000]\n",
      "loss: 0.024947  [ 6464/60000]\n",
      "loss: 0.027589  [12864/60000]\n",
      "loss: 0.020660  [19264/60000]\n",
      "loss: 0.031962  [25664/60000]\n",
      "loss: 0.026338  [32064/60000]\n",
      "loss: 0.023710  [38464/60000]\n",
      "loss: 0.056746  [44864/60000]\n",
      "loss: 0.023956  [51264/60000]\n",
      "loss: 0.025322  [57664/60000]\n",
      "Test Loss: 0.027222 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.022124  [   64/60000]\n",
      "loss: 0.018545  [ 6464/60000]\n",
      "loss: 0.024319  [12864/60000]\n",
      "loss: 0.019797  [19264/60000]\n",
      "loss: 0.030812  [25664/60000]\n",
      "loss: 0.030692  [32064/60000]\n",
      "loss: 0.031518  [38464/60000]\n",
      "loss: 0.023947  [44864/60000]\n",
      "loss: 0.027752  [51264/60000]\n",
      "loss: 0.024816  [57664/60000]\n",
      "Test Loss: 0.027738 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.025296  [   64/60000]\n",
      "loss: 0.029074  [ 6464/60000]\n",
      "loss: 0.020916  [12864/60000]\n",
      "loss: 0.024156  [19264/60000]\n",
      "loss: 0.017128  [25664/60000]\n",
      "loss: 0.018767  [32064/60000]\n",
      "loss: 0.036600  [38464/60000]\n",
      "loss: 0.026149  [44864/60000]\n",
      "loss: 0.018696  [51264/60000]\n",
      "loss: 0.039368  [57664/60000]\n",
      "Test Loss: 0.027579 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.033153  [   64/60000]\n",
      "loss: 0.021056  [ 6464/60000]\n",
      "loss: 0.017247  [12864/60000]\n",
      "loss: 0.033422  [19264/60000]\n",
      "loss: 0.022468  [25664/60000]\n",
      "loss: 0.026913  [32064/60000]\n",
      "loss: 0.025808  [38464/60000]\n",
      "loss: 0.021713  [44864/60000]\n",
      "loss: 0.029748  [51264/60000]\n",
      "loss: 0.018950  [57664/60000]\n",
      "Test Loss: 0.030136 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.025799  [   64/60000]\n",
      "loss: 0.019225  [ 6464/60000]\n",
      "loss: 0.024701  [12864/60000]\n",
      "loss: 0.026474  [19264/60000]\n",
      "loss: 0.022335  [25664/60000]\n",
      "loss: 0.040772  [32064/60000]\n",
      "loss: 0.025953  [38464/60000]\n",
      "loss: 0.031386  [44864/60000]\n",
      "loss: 0.022918  [51264/60000]\n",
      "loss: 0.022193  [57664/60000]\n",
      "Test Loss: 0.027711 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.026029  [   64/60000]\n",
      "loss: 0.037415  [ 6464/60000]\n",
      "loss: 0.029494  [12864/60000]\n",
      "loss: 0.031668  [19264/60000]\n",
      "loss: 0.028027  [25664/60000]\n",
      "loss: 0.029762  [32064/60000]\n",
      "loss: 0.026408  [38464/60000]\n",
      "loss: 0.039181  [44864/60000]\n",
      "loss: 0.022100  [51264/60000]\n",
      "loss: 0.025295  [57664/60000]\n",
      "Test Loss: 0.027591 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.033709  [   64/60000]\n",
      "loss: 0.033139  [ 6464/60000]\n",
      "loss: 0.022694  [12864/60000]\n",
      "loss: 0.028128  [19264/60000]\n",
      "loss: 0.029763  [25664/60000]\n",
      "loss: 0.024132  [32064/60000]\n",
      "loss: 0.017003  [38464/60000]\n",
      "loss: 0.021723  [44864/60000]\n",
      "loss: 0.026127  [51264/60000]\n",
      "loss: 0.038671  [57664/60000]\n",
      "Test Loss: 0.027790 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.029196  [   64/60000]\n",
      "loss: 0.029194  [ 6464/60000]\n",
      "loss: 0.031250  [12864/60000]\n",
      "loss: 0.030004  [19264/60000]\n",
      "loss: 0.034149  [25664/60000]\n",
      "loss: 0.039030  [32064/60000]\n",
      "loss: 0.027941  [38464/60000]\n",
      "loss: 0.020776  [44864/60000]\n",
      "loss: 0.027230  [51264/60000]\n",
      "loss: 0.017033  [57664/60000]\n",
      "Test Loss: 0.027176 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.031701  [   64/60000]\n",
      "loss: 0.019836  [ 6464/60000]\n",
      "loss: 0.022272  [12864/60000]\n",
      "loss: 0.031713  [19264/60000]\n",
      "loss: 0.029010  [25664/60000]\n",
      "loss: 0.029590  [32064/60000]\n",
      "loss: 0.021970  [38464/60000]\n",
      "loss: 0.034611  [44864/60000]\n",
      "loss: 0.036739  [51264/60000]\n",
      "loss: 0.032292  [57664/60000]\n",
      "Test Loss: 0.028389 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.020023  [   64/60000]\n",
      "loss: 0.034239  [ 6464/60000]\n",
      "loss: 0.023019  [12864/60000]\n",
      "loss: 0.039942  [19264/60000]\n",
      "loss: 0.026615  [25664/60000]\n",
      "loss: 0.028926  [32064/60000]\n",
      "loss: 0.028083  [38464/60000]\n",
      "loss: 0.028628  [44864/60000]\n",
      "loss: 0.034522  [51264/60000]\n",
      "loss: 0.027749  [57664/60000]\n",
      "Test Loss: 0.027910 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.021163  [   64/60000]\n",
      "loss: 0.030373  [ 6464/60000]\n",
      "loss: 0.024858  [12864/60000]\n",
      "loss: 0.029336  [19264/60000]\n",
      "loss: 0.022226  [25664/60000]\n",
      "loss: 0.023745  [32064/60000]\n",
      "loss: 0.020074  [38464/60000]\n",
      "loss: 0.034750  [44864/60000]\n",
      "loss: 0.032337  [51264/60000]\n",
      "loss: 0.031216  [57664/60000]\n",
      "Test Loss: 0.028265 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.023593  [   64/60000]\n",
      "loss: 0.015746  [ 6464/60000]\n",
      "loss: 0.028743  [12864/60000]\n",
      "loss: 0.020336  [19264/60000]\n",
      "loss: 0.036173  [25664/60000]\n",
      "loss: 0.017338  [32064/60000]\n",
      "loss: 0.029462  [38464/60000]\n",
      "loss: 0.035712  [44864/60000]\n",
      "loss: 0.035729  [51264/60000]\n",
      "loss: 0.037579  [57664/60000]\n",
      "Test Loss: 0.027121 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.024698  [   64/60000]\n",
      "loss: 0.030565  [ 6464/60000]\n",
      "loss: 0.027179  [12864/60000]\n",
      "loss: 0.025721  [19264/60000]\n",
      "loss: 0.029532  [25664/60000]\n",
      "loss: 0.035603  [32064/60000]\n",
      "loss: 0.022304  [38464/60000]\n",
      "loss: 0.030988  [44864/60000]\n",
      "loss: 0.035098  [51264/60000]\n",
      "loss: 0.034948  [57664/60000]\n",
      "Test Loss: 0.026343 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.025682  [   64/60000]\n",
      "loss: 0.033630  [ 6464/60000]\n",
      "loss: 0.026065  [12864/60000]\n",
      "loss: 0.023703  [19264/60000]\n",
      "loss: 0.027780  [25664/60000]\n",
      "loss: 0.024599  [32064/60000]\n",
      "loss: 0.028423  [38464/60000]\n",
      "loss: 0.022758  [44864/60000]\n",
      "loss: 0.023699  [51264/60000]\n",
      "loss: 0.027790  [57664/60000]\n",
      "Test Loss: 0.027636 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.029104  [   64/60000]\n",
      "loss: 0.041536  [ 6464/60000]\n",
      "loss: 0.028325  [12864/60000]\n",
      "loss: 0.025093  [19264/60000]\n",
      "loss: 0.022205  [25664/60000]\n",
      "loss: 0.026399  [32064/60000]\n",
      "loss: 0.027325  [38464/60000]\n",
      "loss: 0.023034  [44864/60000]\n",
      "loss: 0.029968  [51264/60000]\n",
      "loss: 0.023905  [57664/60000]\n",
      "Test Loss: 0.027124 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.027296  [   64/60000]\n",
      "loss: 0.028858  [ 6464/60000]\n",
      "loss: 0.026367  [12864/60000]\n",
      "loss: 0.025242  [19264/60000]\n",
      "loss: 0.032961  [25664/60000]\n",
      "loss: 0.027865  [32064/60000]\n",
      "loss: 0.030803  [38464/60000]\n",
      "loss: 0.038888  [44864/60000]\n",
      "loss: 0.025629  [51264/60000]\n",
      "loss: 0.020935  [57664/60000]\n",
      "Test Loss: 0.026901 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.024910  [   64/60000]\n",
      "loss: 0.025540  [ 6464/60000]\n",
      "loss: 0.021660  [12864/60000]\n",
      "loss: 0.027519  [19264/60000]\n",
      "loss: 0.033284  [25664/60000]\n",
      "loss: 0.046875  [32064/60000]\n",
      "loss: 0.022642  [38464/60000]\n",
      "loss: 0.035082  [44864/60000]\n",
      "loss: 0.030767  [51264/60000]\n",
      "loss: 0.032458  [57664/60000]\n",
      "Test Loss: 0.026594 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.022112  [   64/60000]\n",
      "loss: 0.028639  [ 6464/60000]\n",
      "loss: 0.027014  [12864/60000]\n",
      "loss: 0.022119  [19264/60000]\n",
      "loss: 0.022551  [25664/60000]\n",
      "loss: 0.032375  [32064/60000]\n",
      "loss: 0.021823  [38464/60000]\n",
      "loss: 0.031747  [44864/60000]\n",
      "loss: 0.023141  [51264/60000]\n",
      "loss: 0.024299  [57664/60000]\n",
      "Test Loss: 0.026987 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.023075  [   64/60000]\n",
      "loss: 0.034237  [ 6464/60000]\n",
      "loss: 0.024265  [12864/60000]\n",
      "loss: 0.030709  [19264/60000]\n",
      "loss: 0.026303  [25664/60000]\n",
      "loss: 0.024676  [32064/60000]\n",
      "loss: 0.030711  [38464/60000]\n",
      "loss: 0.022159  [44864/60000]\n",
      "loss: 0.030186  [51264/60000]\n",
      "loss: 0.029972  [57664/60000]\n",
      "Test Loss: 0.026246 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.029089  [   64/60000]\n",
      "loss: 0.029696  [ 6464/60000]\n",
      "loss: 0.021657  [12864/60000]\n",
      "loss: 0.025039  [19264/60000]\n",
      "loss: 0.024360  [25664/60000]\n",
      "loss: 0.028700  [32064/60000]\n",
      "loss: 0.030989  [38464/60000]\n",
      "loss: 0.040161  [44864/60000]\n",
      "loss: 0.021121  [51264/60000]\n",
      "loss: 0.018745  [57664/60000]\n",
      "Test Loss: 0.026176 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.039328  [   64/60000]\n",
      "loss: 0.025221  [ 6464/60000]\n",
      "loss: 0.020374  [12864/60000]\n",
      "loss: 0.026416  [19264/60000]\n",
      "loss: 0.017668  [25664/60000]\n",
      "loss: 0.030926  [32064/60000]\n",
      "loss: 0.024256  [38464/60000]\n",
      "loss: 0.022532  [44864/60000]\n",
      "loss: 0.027810  [51264/60000]\n",
      "loss: 0.025551  [57664/60000]\n",
      "Test Loss: 0.026783 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(device)\n",
    "\n",
    "model = ScoreNetwork0(channels=1).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "DC3n47TepoXJ",
    "outputId": "3e71ca1b-243c-4802-ab6c-4151435e1e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ebb860e9a50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = sampling(model)\n",
    "sample = sample.detach().cpu().numpy()\n",
    "plt.imshow(np.squeeze(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2bbB8hylH6mu"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YySOeo-IETT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
